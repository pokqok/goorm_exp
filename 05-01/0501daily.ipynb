{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1e0f4dde",
      "metadata": {
        "id": "1e0f4dde"
      },
      "source": [
        "\n",
        "# 🎓 초급 딥러닝 미션:\n",
        "# 딥러닝 학교 – 마법 신경망으로 세계를 구하라!\n",
        "\n",
        "---\n",
        "\n",
        "## 🏰 배경 스토리\n",
        "\n",
        "당신은 **AI 마법학교의 입학생**입니다.  \n",
        "이 학교에서는 사람의 감정, 날씨, 질병을 예측하는 **마법 신경망**을 수련합니다.  \n",
        "당신의 첫 수업은 **딥러닝 기초 수련**입니다.  \n",
        "당신은 NumPy만으로 **마법 신경망**을 직접 구현하고, 다양한 마법서(Optimizer)를 실험하며,  \n",
        "마지막엔 **PyTorch 마법 시스템**에 입문하게 됩니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 미션 학습 목표\n",
        "\n",
        "- 딥러닝 구조 및 원리 이해 (forward, loss, backward)\n",
        "- 학습률 개념 실험\n",
        "- 옵티마이저 종류 비교 (SGD, Momentum, RMSProp, Adam)\n",
        "- 파이토치 기초 문법 학습 및 비교\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 미션 구성\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95496ac7",
      "metadata": {
        "id": "95496ac7"
      },
      "source": [
        "\n",
        "### ✅ Part 1. 딥러닝 개요 퀴즈\n",
        "\n",
        "1. 딥러닝에서 '신경망'이란 무엇을 모방한 것인가요?  \n",
        "   a. 컴퓨터 회로  \n",
        "   b. 인간 뇌의 뉴런 구조  \n",
        "   c. 수학 공식  \n",
        "   d. 로봇 제어기\n",
        "\n",
        "2. Backpropagation은 무엇을 계산하는 과정인가요?  \n",
        "   a. 데이터 전처리  \n",
        "   b. 예측 결과 시각화  \n",
        "   c. 손실 함수의 그래디언트를 역방향으로 계산  \n",
        "   d. 예측값을 정규화하는 함수\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b683bf32",
      "metadata": {
        "id": "b683bf32"
      },
      "source": [
        "b,c"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f93b89c4",
      "metadata": {
        "id": "f93b89c4"
      },
      "source": [
        "\n",
        "### ✅ Part 2. NumPy 기반 신경망 실습\n",
        "\n",
        "#### 🎯 문제: XOR 문제를 NumPy만으로 푸는 2층 신경망을 직접 구현해보세요.\n",
        "\n",
        "- 입력: 2차원 벡터 (ex: [0,1], [1,1])\n",
        "- 출력: 0 또는 1\n",
        "- 구조: 입력(2) → 은닉층(4, ReLU) → 출력층(1, Sigmoid)\n",
        "- 손실 함수: Binary Cross Entropy\n",
        "\n",
        "#### ✏️ 구현 항목\n",
        "1. forward() – 순전파 계산\n",
        "2. compute_loss() – 손실 계산\n",
        "3. backward() – 역전파 계산 (Chain Rule 기반)\n",
        "4. update_weights() – SGD 기반 파라미터 갱신 ==> 중요함!!\n",
        "5. 학습률 변화 실험 (0.001 / 0.01 / 0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeb63b68",
      "metadata": {
        "id": "aeb63b68",
        "outputId": "f99c3b1b-0076-4588-a548-dcbe4ac6a7f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 1.093064599598056\n",
            "Epoch 1000, Loss: 0.6952855339915367\n",
            "Epoch 2000, Loss: 0.6936971588677859\n",
            "Epoch 3000, Loss: 0.6932910022123189\n",
            "Epoch 4000, Loss: 0.6931849512003239\n",
            "Epoch 5000, Loss: 0.6931571110298702\n",
            "Epoch 6000, Loss: 0.6931497921969233\n",
            "Epoch 7000, Loss: 0.6931478674533718\n",
            "Epoch 8000, Loss: 0.6931473612252397\n",
            "Epoch 9000, Loss: 0.6931472280784128\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class XORNetwork:\n",
        "    def __init__(self, input_size=2, hidden_size=4):\n",
        "        # 가중치 초기화\n",
        "        self.W1 = np.random.randn(input_size, hidden_size)\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, 1)\n",
        "        self.b2 = np.zeros((1, 1))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # 순전파\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.relu(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def compute_loss(self, Y, Y_pred):\n",
        "        # Binary Cross Entropy 손실\n",
        "        return -np.mean(Y * np.log(Y_pred) + (1 - Y) * np.log(1 - Y_pred))\n",
        "\n",
        "    def backward(self, X, Y):\n",
        "        # 역전파 구현\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # 출력층 오차\n",
        "        dZ2 = self.a2 - Y\n",
        "        dW2 = (1/m) * np.dot(self.a1.T, dZ2)\n",
        "        db2 = (1/m) * np.sum(dZ2, axis=0)\n",
        "\n",
        "        # 은닉층 오차\n",
        "        dZ1 = np.dot(dZ2, self.W2.T) * (self.z1 > 0)\n",
        "        dW1 = (1/m) * np.dot(X.T, dZ1)\n",
        "        db1 = (1/m) * np.sum(dZ1, axis=0)\n",
        "\n",
        "        return dW1, db1, dW2, db2\n",
        "\n",
        "    def update_weights(self, dW1, db1, dW2, db2, learning_rate=0.01):\n",
        "        # SGD 가중치 업데이트\n",
        "        self.W1 -= learning_rate * dW1\n",
        "        self.b1 -= learning_rate * db1\n",
        "        self.W2 -= learning_rate * dW2\n",
        "        self.b2 -= learning_rate * db2\n",
        "\n",
        "# 학습\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "Y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "model = XORNetwork()\n",
        "epochs = 10000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # 순전파\n",
        "    Y_pred = model.forward(X)\n",
        "\n",
        "    # 손실 계산\n",
        "    loss = model.compute_loss(Y, Y_pred)\n",
        "\n",
        "    # 역전파\n",
        "    dW1, db1, dW2, db2 = model.backward(X, Y)\n",
        "\n",
        "    # 가중치 업데이트\n",
        "    model.update_weights(dW1, db1, dW2, db2)\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9721a31",
      "metadata": {
        "id": "a9721a31"
      },
      "source": [
        "\n",
        "### ✅ Part 3. 다양한 옵티마이저 실험 (NumPy 구현 or 의사코드)\n",
        "\n",
        "| Optimizer | 설명 |\n",
        "|-----------|------|\n",
        "| SGD | 가장 기본적인 경사하강법 |\n",
        "| Momentum | 이전 기울기를 누적하여 반동 효과 부여 |\n",
        "| RMSProp | 각 파라미터별 적응적 학습률 적용 |\n",
        "| Adam | Momentum + RMSProp을 결합한 대표적 옵티마이저 |\n",
        "\n",
        "\n",
        "아래 코드는 어떤 방식의 옵티마이저의 구현인지 답하시오.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7f9a508",
      "metadata": {
        "id": "f7f9a508"
      },
      "outputs": [],
      "source": [
        "# 💡 간단한 옵티마이저 의사코드 예시임.\n",
        "v = 0\n",
        "v = beta * v + (1 - beta) * grad\n",
        "w = w - learning_rate * v\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04fa9ebf",
      "metadata": {
        "id": "04fa9ebf"
      },
      "source": [
        "모멘텀 옵티마이저"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea19d8e3",
      "metadata": {
        "id": "ea19d8e3"
      },
      "source": [
        "\n",
        "### ✅ Part 4. PyTorch 입문 과제\n",
        "\n",
        "이제부터는 AI 마법학교의 공식 프레임워크인 **PyTorch**를 배웁니다!\n",
        "\n",
        "#### 🎯 미션: 위 XOR 문제를 PyTorch로 다시 풀어보세요.\n",
        "- PyTorch로 간단한 학습 루프 예제 코드를 검색해서 찾아서 직접 작성해보세요.\n",
        "- torch.nn.Linear, torch.ReLU, torch.Sigmoid, torch.BCELoss 등 사용\n",
        "- torch.optim.SGD, Adam, RMSprop 등 Optimizer로 비교\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "93ee365b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93ee365b",
        "outputId": "e23d512c-bca3-42ba-885b-adbc1f3369dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [500/5000], Loss: 0.0221\n",
            "Epoch [1000/5000], Loss: 0.0061\n",
            "Epoch [1500/5000], Loss: 0.0028\n",
            "Epoch [2000/5000], Loss: 0.0016\n",
            "Epoch [2500/5000], Loss: 0.0010\n",
            "Epoch [3000/5000], Loss: 0.0007\n",
            "Epoch [3500/5000], Loss: 0.0005\n",
            "Epoch [4000/5000], Loss: 0.0003\n",
            "Epoch [4500/5000], Loss: 0.0002\n",
            "Epoch [5000/5000], Loss: 0.0002\n",
            "\n",
            "최종 정확도: 100.00%\n",
            "\n",
            "예측 결과:\n",
            "입력: [0. 0.], 실제값: [0.], 예측값: [0.]\n",
            "입력: [0. 1.], 실제값: [1.], 예측값: [1.]\n",
            "입력: [1. 0.], 실제값: [1.], 예측값: [1.]\n",
            "입력: [1. 1.], 실제값: [0.], 예측값: [0.]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# 데이터 준비\n",
        "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
        "Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
        "\n",
        "# 모델 정의\n",
        "class XORNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(XORNetwork, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2, 4),  # 입력층 2개 → 은닉층 4개\n",
        "            nn.ReLU(),         # ReLU 활성화 함수\n",
        "            nn.Linear(4, 1),   # 은닉층 4개 → 출력층 1개\n",
        "            nn.Sigmoid()       # 이진 분류를 위한 시그모이드\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 모델, 손실함수, 옵티마이저 설정\n",
        "model = XORNetwork()\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 학습 루프\n",
        "epochs = 5000\n",
        "for epoch in range(epochs):\n",
        "    # 순전파\n",
        "    outputs = model(X)\n",
        "\n",
        "    # 손실 계산\n",
        "    loss = criterion(outputs, Y)\n",
        "\n",
        "    # 역전파 및 가중치 갱신\n",
        "    optimizer.zero_grad()  # 기울기 초기화\n",
        "    loss.backward()        # 역전파\n",
        "    optimizer.step()       # 가중치 업데이트\n",
        "\n",
        "    # 일정 주기로 손실 출력\n",
        "    if (epoch + 1) % 500 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# 최종 예측 및 정확도 평가\n",
        "with torch.no_grad():\n",
        "    predictions = (model(X) > 0.5).float()\n",
        "    accuracy = (predictions == Y).float().mean()\n",
        "    print(f'\\n최종 정확도: {accuracy.item() * 100:.2f}%')\n",
        "\n",
        "    # 결과 출력\n",
        "    print(\"\\n예측 결과:\")\n",
        "    for i in range(len(X)):\n",
        "        print(f\"입력: {X[i].numpy()}, 실제값: {Y[i].numpy()}, 예측값: {predictions[i].numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac0d7cd4",
      "metadata": {
        "id": "ac0d7cd4"
      },
      "source": [
        "\n",
        "## 🏁 마무리 퀴즈\n",
        "\n",
        "1. 학습률이 너무 크면 발생할 수 있는 문제는?\n",
        "   - a. 과적합  \n",
        "   - b. 손실 발산  \n",
        "   - c. 더 정밀한 학습  \n",
        "   - d. 정확도가 무조건 올라감\n",
        "\n",
        "2. Adam 옵티마이저는 어떤 두 가지 개념을 결합한 것인가요?\n",
        "   - a. LSTM + GRU  \n",
        "   - b. SGD + Dropout  \n",
        "   - c. Momentum + RMSProp  \n",
        "   - d. SGD + AdaGrad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b, c"
      ],
      "metadata": {
        "id": "58mWqR1gp19L"
      },
      "id": "58mWqR1gp19L"
    },
    {
      "cell_type": "markdown",
      "id": "6271ac05",
      "metadata": {
        "id": "6271ac05"
      },
      "source": [
        "\n",
        "## 🎁 보너스 미션 (선택)\n",
        "\n",
        "- 직접 XOR이 아닌 make_moons, make_circles 등의 데이터셋으로 실험해보세요.\n",
        "- 은닉층의 노드 수를 바꿔가며 학습 성능을 비교해보세요.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "98b17132",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98b17132",
        "outputId": "d65ed45b-f62e-4a6d-925e-dc5e6eec6e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MOONS 데이터셋 실험:\n",
            "은닉층 [4]: 정확도 85.00%\n",
            "은닉층 [4, 4]: 정확도 99.00%\n",
            "은닉층 [8]: 정확도 100.00%\n",
            "은닉층 [8, 8]: 정확도 100.00%\n",
            "은닉층 [16]: 정확도 100.00%\n",
            "\n",
            "CIRCLES 데이터셋 실험:\n",
            "은닉층 [4]: 정확도 87.00%\n",
            "은닉층 [4, 4]: 정확도 83.00%\n",
            "은닉층 [8]: 정확도 85.00%\n",
            "은닉층 [8, 8]: 정확도 85.00%\n",
            "은닉층 [16]: 정확도 85.00%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_moons, make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터셋 생성 및 전처리 함수\n",
        "def prepare_dataset(dataset_type='moons', test_size=0.2):\n",
        "    # 데이터셋 생성\n",
        "    if dataset_type == 'moons':\n",
        "        X, y = make_moons(n_samples=500, noise=0.15, random_state=42)\n",
        "    else:  # circles\n",
        "        X, y = make_circles(n_samples=500, noise=0.1, random_state=42)\n",
        "\n",
        "    # 데이터 분할\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=42\n",
        "    )\n",
        "\n",
        "    # 표준화\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # NumPy → PyTorch 텐서 변환\n",
        "    X_train = torch.FloatTensor(X_train)\n",
        "    X_test = torch.FloatTensor(X_test)\n",
        "    y_train = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "    y_test = torch.FloatTensor(y_test).unsqueeze(1)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# 신경망 모델 클래스\n",
        "class FlexibleNetwork(nn.Module):\n",
        "    def __init__(self, input_size=2, hidden_sizes=[4, 4], output_size=1):\n",
        "        super(FlexibleNetwork, self).__init__()\n",
        "\n",
        "        # 동적으로 레이어 생성\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "\n",
        "        # 은닉층 추가\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(prev_size, hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            prev_size = hidden_size\n",
        "\n",
        "        # 출력층\n",
        "        layers.append(nn.Linear(prev_size, output_size))\n",
        "        layers.append(nn.Sigmoid())\n",
        "\n",
        "        # 순차 모델 생성\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 학습 및 평가 함수\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test, hidden_sizes=[4, 4], lr=0.01, epochs=1000):\n",
        "    # 모델, 손실함수, 옵티마이저 설정\n",
        "    model = FlexibleNetwork(hidden_sizes=hidden_sizes)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # 학습\n",
        "    for epoch in range(epochs):\n",
        "        # 순전파\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        # 역전파\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # 평가\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(X_test)\n",
        "        predictions = (test_outputs > 0.5).float()\n",
        "        accuracy = (predictions == y_test).float().mean()\n",
        "\n",
        "    return accuracy.item()\n",
        "\n",
        "# 데이터셋별, 은닉층 구조별 실험\n",
        "def experiment():\n",
        "    # 실험할 데이터셋 유형\n",
        "    datasets = ['moons', 'circles']\n",
        "\n",
        "    # 실험할 은닉층 구조들\n",
        "    hidden_layer_configs = [\n",
        "        [4],           # 1개 은닉층, 4노드\n",
        "        [4, 4],        # 2개 은닉층, 각 4노드\n",
        "        [8],           # 1개 은닉층, 8노드\n",
        "        [8, 8],        # 2개 은닉층, 각 8노드\n",
        "        [16],          # 1개 은닉층, 16노드\n",
        "    ]\n",
        "\n",
        "    # 결과 저장\n",
        "    results = {}\n",
        "\n",
        "    # 각 데이터셋에 대해 실험\n",
        "    for dataset in datasets:\n",
        "        print(f\"\\n{dataset.upper()} 데이터셋 실험:\")\n",
        "\n",
        "        # 데이터 준비\n",
        "        X_train, X_test, y_train, y_test = prepare_dataset(dataset)\n",
        "\n",
        "        # 각 은닉층 구조로 실험\n",
        "        dataset_results = {}\n",
        "        for config in hidden_layer_configs:\n",
        "            accuracy = train_and_evaluate(\n",
        "                X_train, X_test, y_train, y_test,\n",
        "                hidden_sizes=config\n",
        "            )\n",
        "            dataset_results[str(config)] = accuracy\n",
        "            print(f\"은닉층 {config}: 정확도 {accuracy*100:.2f}%\")\n",
        "\n",
        "        results[dataset] = dataset_results\n",
        "\n",
        "    return results\n",
        "\n",
        "# 실험 및 시각화\n",
        "results = experiment()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2d09400",
      "metadata": {
        "id": "f2d09400"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}