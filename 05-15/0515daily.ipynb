{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. 데이터 로드\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLTK 리소스 다운로드\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pig8udKfkMpN",
        "outputId": "b7e6b222-aeac-4bf7-f2e0-78bd3163a74e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEFvZRKTjO7l",
        "outputId": "ea9900a8-ce58-4719-c232-a46c2825ff89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "긍정 트윗 개수: 1000\n",
            "부정 트윗 개수: 1000\n",
            "\n",
            "긍정 트윗 예시:\n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "부정 트윗 예시:\n",
            " hopeless for tmr :(\n"
          ]
        }
      ],
      "source": [
        "# 긍정 및 부정 트윗 로드\n",
        "pos_tweets = twitter_samples.strings('positive_tweets.json')[:1000]\n",
        "neg_tweets = twitter_samples.strings('negative_tweets.json')[:1000]\n",
        "\n",
        "print(\"긍정 트윗 개수:\", len(pos_tweets))\n",
        "print(\"부정 트윗 개수:\", len(neg_tweets))\n",
        "print(\"\\n긍정 트윗 예시:\\n\", pos_tweets[0])\n",
        "print(\"\\n부정 트윗 예시:\\n\", neg_tweets[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 텍스트 전처리\n",
        "def preprocess_text(text):\n",
        "    # 소문자화\n",
        "    text = text.lower()\n",
        "\n",
        "    # URL 제거\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # 특수문자, 숫자 제거\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # 연속된 공백 제거\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # 축약어 확장\n",
        "    text = text.replace(\"n't\", \" not\")\n",
        "    text = text.replace(\"'m\", \" am\")\n",
        "    text = text.replace(\"'s\", \" is\")\n",
        "    text = text.replace(\"'re\", \" are\")\n",
        "    text = text.replace(\"'ll\", \" will\")\n",
        "    text = text.replace(\"wanna\", \"want to\")\n",
        "    text = text.replace(\"gonna\", \"going to\")\n",
        "\n",
        "    return text\n",
        "\n",
        "# 전처리 적용\n",
        "pos_cleaned = [preprocess_text(tweet) for tweet in pos_tweets]\n",
        "neg_cleaned = [preprocess_text(tweet) for tweet in neg_tweets]\n",
        "\n",
        "print(\"원본 트윗:\\n\", pos_tweets[0])\n",
        "print(\"\\n전처리된 트윗:\\n\", pos_cleaned[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMSk-9qAkWwA",
        "outputId": "1891e19e-ea8c-4f88-9c05-2ec46d98a382"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본 트윗:\n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "전처리된 트윗:\n",
            " followfriday franceinte pkuchly milipolparis for being top engaged members in my community this week\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 토큰화\n",
        "# 샘플 5개 문장 토큰화\n",
        "sample_sentences = pos_cleaned[:5]\n",
        "\n",
        "# 문장 토큰화\n",
        "sent_tokens = [sent_tokenize(sent) for sent in sample_sentences]\n",
        "\n",
        "# 단어 토큰화\n",
        "word_tokens = [word_tokenize(sent[0]) for sent in sent_tokens]\n",
        "\n",
        "print(\"문장 토큰화 결과:\")\n",
        "for i, tokens in enumerate(sent_tokens, 1):\n",
        "    print(f\"{i}. {tokens}\")\n",
        "\n",
        "print(\"\\n단어 토큰화 결과 (첫 10개 토큰):\")\n",
        "for i, tokens in enumerate(word_tokens, 1):\n",
        "    print(f\"{i}. {tokens[:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoG0-3pFkc07",
        "outputId": "16941ef4-e96a-4bd9-d2b5-f084bd2d833a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 토큰화 결과:\n",
            "1. ['followfriday franceinte pkuchly milipolparis for being top engaged members in my community this week']\n",
            "2. ['lambja hey james how odd please call our contact centre on and we will be able to assist you many thanks']\n",
            "3. ['despiteofficial we had a listen last night as you bleed is an amazing track when are you in scotland']\n",
            "4. ['sides congrats']\n",
            "5. ['yeaaaah yippppy my accnt verified rqst has succeed got a blue tick mark on my fb profile in days']\n",
            "\n",
            "단어 토큰화 결과 (첫 10개 토큰):\n",
            "1. ['followfriday', 'franceinte', 'pkuchly', 'milipolparis', 'for', 'being', 'top', 'engaged', 'members', 'in']\n",
            "2. ['lambja', 'hey', 'james', 'how', 'odd', 'please', 'call', 'our', 'contact', 'centre']\n",
            "3. ['despiteofficial', 'we', 'had', 'a', 'listen', 'last', 'night', 'as', 'you', 'bleed']\n",
            "4. ['sides', 'congrats']\n",
            "5. ['yeaaaah', 'yippppy', 'my', 'accnt', 'verified', 'rqst', 'has', 'succeed', 'got', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 불용어 제거\n",
        "# NLTK 영어 불용어 로드\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# 추가 불용어 정의\n",
        "additional_stopwords = {\n",
        "    'rt', 'via', 'omg', 'lol', 'wow',\n",
        "    'wanna', 'gonna', 'gotta', 'ain\\'t', 'y\\'all', 'im', 'u', 'jnlazts', 'amp'}\n",
        "\n",
        "stop_words.update(additional_stopwords)\n",
        "\n",
        "# 불용어 제거 함수\n",
        "def remove_stopwords(tokens):\n",
        "    return [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# 불용어 제거 적용\n",
        "pos_tokens = [remove_stopwords(word_tokenize(tweet)) for tweet in pos_cleaned]\n",
        "neg_tokens = [remove_stopwords(word_tokenize(tweet)) for tweet in neg_cleaned]\n",
        "\n",
        "print(\"불용어 제거 전 (첫 번째 트윗):\", word_tokenize(pos_cleaned[0]))\n",
        "print(\"불용어 제거 후 (첫 번째 트윗):\", pos_tokens[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOvddyk4m79w",
        "outputId": "8c745db1-a3f1-4a84-acdc-78b9409c3457"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 (첫 번째 트윗): ['followfriday', 'franceinte', 'pkuchly', 'milipolparis', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week']\n",
            "불용어 제거 후 (첫 번째 트윗): ['followfriday', 'franceinte', 'pkuchly', 'milipolparis', 'top', 'engaged', 'members', 'community', 'week']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 단어 사전 구축 (Vocabulary)\n",
        "from collections import Counter\n",
        "\n",
        "# 전체 토큰 병합\n",
        "all_tokens = [token for tokens in pos_tokens + neg_tokens for token in tokens]\n",
        "\n",
        "# 단어 빈도수 계산\n",
        "word_freq = Counter(all_tokens)\n",
        "\n",
        "# 상위 5,000개 단어 선택\n",
        "vocab = {word: idx for idx, (word, _) in enumerate(word_freq.most_common(5000), 1)}\n",
        "\n",
        "# 결과 출력\n",
        "print(\"단어 사전 크기:\", len(vocab))\n",
        "print(\"\\n상위 20개 단어와 빈도:\")\n",
        "for word, freq in word_freq.most_common(20):\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ycj4AllKnM9n",
        "outputId": "a5b1340c-7511-4a50-dec8-4cbb2895032f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 사전 크기: 5000\n",
            "\n",
            "상위 20개 단어와 빈도:\n",
            "follow: 127\n",
            "like: 82\n",
            "love: 76\n",
            "want: 74\n",
            "thanks: 72\n",
            "dont: 72\n",
            "cant: 72\n",
            "back: 70\n",
            "good: 67\n",
            "get: 62\n",
            "time: 60\n",
            "know: 60\n",
            "day: 58\n",
            "hi: 53\n",
            "one: 50\n",
            "see: 50\n",
            "going: 48\n",
            "miss: 48\n",
            "thank: 47\n",
            "lt: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 정수 인코딩과 패딩\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 정수 인코딩 함수\n",
        "def encode_tokens(tokens):\n",
        "    return [vocab.get(token, 0) for token in tokens]\n",
        "\n",
        "# 정수 인코딩 적용\n",
        "pos_encoded = [encode_tokens(tokens) for tokens in pos_tokens]\n",
        "neg_encoded = [encode_tokens(tokens) for tokens in neg_tokens]\n",
        "\n",
        "# 패딩\n",
        "max_len = 50\n",
        "pos_padded = pad_sequences(pos_encoded, maxlen=max_len, padding='post')\n",
        "neg_padded = pad_sequences(neg_encoded, maxlen=max_len, padding='post')\n",
        "\n",
        "print(\"패딩 전 첫 번째 트윗:\", pos_encoded[0])\n",
        "print(\"패딩 후 첫 번째 트윗:\", pos_padded[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23_wa8E9nM2a",
        "outputId": "75ed5197-9635-43fc-f4f1-3ee4c43ef11d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "패딩 전 첫 번째 트윗: [63, 1267, 1268, 1269, 44, 294, 295, 51, 33]\n",
            "패딩 후 첫 번째 트윗: [  63 1267 1268 1269   44  294  295   51   33    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. 벡터화\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# 원본 텍스트 재결합\n",
        "pos_texts = [' '.join(tokens) for tokens in pos_tokens]\n",
        "neg_texts = [' '.join(tokens) for tokens in neg_tokens]\n",
        "\n",
        "# Bag-of-Words\n",
        "bow_vectorizer = CountVectorizer(max_features=1000)\n",
        "bow_matrix = bow_vectorizer.fit_transform(pos_texts + neg_texts)\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(pos_texts + neg_texts)\n",
        "\n",
        "print(\"Bag-of-Words 희소 행렬 형태:\", bow_matrix.shape)\n",
        "print(\"TF-IDF 희소 행렬 형태:\", tfidf_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yqd-2bDWnqkr",
        "outputId": "7cb9d4ce-4aa8-4c0e-bfa7-39b60387a9b6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words 희소 행렬 형태: (2000, 1000)\n",
            "TF-IDF 희소 행렬 형태: (2000, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8, 9. Bigram 모델 실습\n",
        "from collections import Counter\n",
        "\n",
        "# 긍정/부정 토큰 병합\n",
        "pos_all_tokens = [token for tokens in pos_tokens for token in tokens]\n",
        "neg_all_tokens = [token for tokens in neg_tokens for token in tokens]\n",
        "\n",
        "# Unigram, Bigram 카운트\n",
        "unigram_pos = Counter(pos_all_tokens)\n",
        "bigram_pos = Counter(zip(pos_all_tokens[:-1], pos_all_tokens[1:]))\n",
        "unigram_neg = Counter(neg_all_tokens)\n",
        "bigram_neg = Counter(zip(neg_all_tokens[:-1], neg_all_tokens[1:]))\n",
        "\n",
        "# Laplace 스무딩 적용\n",
        "alpha = 1\n",
        "V_pos = len(unigram_pos)\n",
        "V_neg = len(unigram_neg)\n",
        "\n",
        "# 예시 조건부 확률 계산\n",
        "def calculate_smoothed_prob(bigram, unigram, prev_word, next_word, V):\n",
        "    return (bigram[(prev_word, next_word)] + alpha) / (unigram[prev_word] + alpha * V)\n",
        "\n",
        "# 상위 10개 Bigram 비교\n",
        "print(\"Top 10 Positive Bigrams:\", bigram_pos.most_common(10))\n",
        "print(\"Top 10 Negative Bigrams:\", bigram_neg.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxFMGIBmns5H",
        "outputId": "960242ae-41ee-445b-ba19-7e258b984fa3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Positive Bigrams: [(('follow', 'back'), 42), (('follow', 'follow'), 37), (('community', 'week'), 22), (('hi', 'bam'), 13), (('bam', 'barsandmelody'), 13), (('barsandmelody', 'follow'), 13), (('follow', 'bestfriend'), 13), (('bestfriend', 'horan'), 13), (('horan', 'loves'), 13), (('loves', 'lot'), 13)]\n",
            "Top 10 Negative Bigrams: [(('want', 'go'), 11), (('dont', 'want'), 11), (('dont', 'know'), 11), (('uniteblue', 'tcot'), 8), (('feel', 'bad'), 6), (('goodbye', 'stage'), 6), (('cant', 'sleep'), 6), (('ice', 'cream'), 6), (('go', 'home'), 5), (('climatechange', 'cc'), 5)]\n"
          ]
        }
      ]
    }
  ]
}