{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUKTIr5uvMBF",
        "outputId": "5dee6174-87a6-408e-a775-2e7a7df11c30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ device: cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet torch torchvision transformers sentencepiece\n",
        "import torch, torch.nn as nn, itertools, math, random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"✅ device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Pig Latin 변환 함수 작성\n",
        "def to_pig_latin(word):\n",
        "    \"\"\"단어 하나를 Pig Latin 으로 변환\"\"\"\n",
        "    vowels = \"aeiou\"\n",
        "    if word[0] in vowels:\n",
        "        return word + \"yay\"\n",
        "    for i, c in enumerate(word):\n",
        "        if c in vowels:\n",
        "            return word[i:] + word[:i] + \"ay\"\n",
        "    return word + \"ay\"  # 만약 모음이 없다면 (예외적 상황)\n",
        "\n",
        "def eng2pig(sentence):\n",
        "    return \" \".join(to_pig_latin(w) for w in sentence.lower().split())\n",
        "\n",
        "base_sentences = [\n",
        "    \"hello world\",\n",
        "    \"i love machine learning\",\n",
        "    \"language models are fun\",\n",
        "    \"space tourism is booming\",\n",
        "    \"beam me up scotty\",\n",
        "    \"the future is bright\",\n",
        "]\n",
        "sentences = base_sentences * 15\n",
        "random.shuffle(sentences)\n",
        "\n",
        "pairs = [(s, eng2pig(s)) for s in sentences]\n",
        "print(\"샘플:\", pairs[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCiJZW-IwD4v",
        "outputId": "23f0cfe2-79ca-46fb-b644-e5cce50cd7a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플: ('space tourism is booming', 'acespay ourismtay isyay oomingbay')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    def __init__(self, sents, specials=[\"<pad>\",\"<sos>\",\"<eos>\",\"<unk>\"]):\n",
        "        tokens = list(itertools.chain.from_iterable(s.split() for s in sents))\n",
        "        uniq = specials + sorted(set(tokens))\n",
        "        self.stoi = {t:i for i,t in enumerate(uniq)}\n",
        "        self.itos = {i:t for t,i in self.stoi.items()}\n",
        "    def encode(self, sent):\n",
        "      return [self.stoi.get(tok, self.stoi[\"<unk>\"]) for tok in sent.split()]\n",
        "    def decode(self, ids):\n",
        "      # 특수 토큰(<pad>, <sos>, <eos>)는 결과에서 제외\n",
        "      return \" \".join(self.itos[i] for i in ids if self.itos[i] not in [\"<pad>\", \"<sos>\", \"<eos>\"])\n",
        "\n",
        "\n",
        "src_vocab = Vocab([s for s,_ in pairs])\n",
        "tgt_vocab = Vocab([t for _,t in pairs])\n",
        "MAX_LEN = max(len(s.split()) for s,_ in pairs) + 2\n",
        "\n",
        "class PigDataset(Dataset):\n",
        "    def __len__(self): return len(pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = pairs[idx]\n",
        "        src_ids = [src_vocab.stoi[\"<sos>\"]] + src_vocab.encode(src) + [src_vocab.stoi[\"<eos>\"]]\n",
        "        tgt_ids = [tgt_vocab.stoi[\"<sos>\"]] + tgt_vocab.encode(tgt) + [tgt_vocab.stoi[\"<eos>\"]]\n",
        "\n",
        "        # padding\n",
        "        src_ids += [src_vocab.stoi[\"<pad>\"]] * (MAX_LEN - len(src_ids))\n",
        "        tgt_ids += [tgt_vocab.stoi[\"<pad>\"]] * (MAX_LEN - len(tgt_ids))\n",
        "\n",
        "        # mask\n",
        "        src_mask = [1 if tok != src_vocab.stoi[\"<pad>\"] else 0 for tok in src_ids]\n",
        "\n",
        "        return torch.tensor(src_ids), torch.tensor(tgt_ids), torch.tensor(src_mask)\n",
        "\n",
        "\n",
        "loader = DataLoader(PigDataset(), batch_size=32, shuffle=True)\n",
        "print(\"어휘 크기:\", len(src_vocab.stoi))\n",
        "\n",
        "for src, tgt, mask in loader:\n",
        "    print(\" src:\", src.shape)\n",
        "    print(\" tgt:\", tgt.shape)\n",
        "    print(\" mask:\", mask.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BE0EGXpw-FE",
        "outputId": "9c5275d2-8f0b-4f3e-9e7d-bcf01ab55c55"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어휘 크기: 25\n",
            " src: torch.Size([32, 6])\n",
            " tgt: torch.Size([32, 6])\n",
            " mask: torch.Size([32, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd\n",
        "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "\n",
        "class Translator(nn.Module):\n",
        "    def __init__(self, sv, tv, d=128, nhead=4, nl=2):\n",
        "        super().__init__()\n",
        "        self.se = nn.Embedding(sv, d)\n",
        "        self.te = nn.Embedding(tv, d)\n",
        "        self.pe = PositionalEncoding(d)\n",
        "        self.tr = nn.Transformer(\n",
        "            d_model=d,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=nl,\n",
        "            num_decoder_layers=nl,\n",
        "            dim_feedforward=512,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(d, tv)\n",
        "\n",
        "    def pad_mask(self, seq, pad):\n",
        "        return seq.eq(pad)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_pad = self.pad_mask(src, src_vocab.stoi[\"<pad>\"])\n",
        "        tgt_pad = self.pad_mask(tgt, tgt_vocab.stoi[\"<pad>\"])\n",
        "\n",
        "        tgt_len = tgt.size(1)\n",
        "        tgt_mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1).bool().to(device)\n",
        "\n",
        "        src = self.pe(self.se(src))\n",
        "        tgt = self.pe(self.te(tgt))\n",
        "\n",
        "        out = self.tr(\n",
        "            src, tgt,\n",
        "            src_key_padding_mask=src_pad,\n",
        "            tgt_key_padding_mask=tgt_pad,\n",
        "            memory_key_padding_mask=src_pad,\n",
        "            tgt_mask=tgt_mask\n",
        "        )\n",
        "        return self.fc(out)\n",
        "\n",
        "\n",
        "model = Translator(len(src_vocab.stoi), len(tgt_vocab.stoi)).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "crit = nn.CrossEntropyLoss(ignore_index=tgt_vocab.stoi[\"<pad>\"])\n",
        "\n",
        "src, tgt, mask = next(iter(loader))\n",
        "out = model(src.to(device), tgt.to(device))\n",
        "print(\"출력 shape:\", out.shape)  # (batch_size, seq_len, vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMPLYPfTxW_l",
        "outputId": "b0c20182-d2a2-4f7c-dde4-21de2cf9c648"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "출력 shape: torch.Size([32, 6, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch():\n",
        "    model.train()\n",
        "    total = 0\n",
        "\n",
        "    for src, tgt, _ in loader:  # src_mask는 사용하지 않으므로 _로 처리\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        # 디코더 입력/출력 분리\n",
        "        tgt_in = tgt[:, :-1]\n",
        "        tgt_out = tgt[:, 1:]\n",
        "\n",
        "        # 모델 출력\n",
        "        logits = model(src, tgt_in)\n",
        "        loss = crit(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total += loss.item()\n",
        "\n",
        "    return total / len(loader)\n",
        "\n",
        "\n",
        "for epoch in range(1, 4):\n",
        "    print(f\"Epoch {epoch} | loss {train_epoch():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTvAS_8FyVLF",
        "outputId": "1876a0dd-20de-46f6-b6ae-90970f714b2b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | loss 3.061\n",
            "Epoch 2 | loss 2.026\n",
            "Epoch 3 | loss 1.219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # 인코더 입력 준비\n",
        "        src_ids = [src_vocab.stoi[\"<sos>\"]] + src_vocab.encode(sentence.lower()) + [src_vocab.stoi[\"<eos>\"]]\n",
        "        src_ids += [src_vocab.stoi[\"<pad>\"]] * (MAX_LEN - len(src_ids))\n",
        "        src = torch.tensor([src_ids]).to(device)  # (1, MAX_LEN)\n",
        "\n",
        "        # 디코더 입력 시작: <sos>\n",
        "        tgt_ids = [tgt_vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "        for _ in range(MAX_LEN):\n",
        "            tgt = torch.tensor([tgt_ids]).to(device)  # (1, cur_len)\n",
        "            logits = model(src, tgt)  # (1, cur_len, vocab_size)\n",
        "            next_id = logits[0, -1].argmax().item()  # 마지막 토큰의 예측\n",
        "\n",
        "            # 종료 조건\n",
        "            if next_id == tgt_vocab.stoi[\"<eos>\"]:\n",
        "                break\n",
        "            tgt_ids.append(next_id)\n",
        "\n",
        "        return tgt_vocab.decode(tgt_ids[1:])  # <sos> 제외\n",
        "\n",
        "print(translate(\"hello world\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovkeItzuzDCC",
        "outputId": "ee229916-7f8c-41cd-d014-36263f12520e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ellohay orldway\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# TODO: 파이프라인 초기화\n",
        "sentiment = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "samples = [  # 감정 분석할 문장 2개\n",
        "    \"I love this space travel experience!\",\n",
        "    \"She's my darling wife.\",\n",
        "    \"fucking service\",\n",
        "    \"soso\"\n",
        "]\n",
        "\n",
        "for s in samples:\n",
        "    print(s, sentiment(s))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOfe04kT1oqF",
        "outputId": "4889eac8-c646-4f43-ee8c-d2d3ad12bdc7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love this space travel experience! [{'label': '5 stars', 'score': 0.8984273672103882}]\n",
            "She's my darling wife. [{'label': '5 stars', 'score': 0.7538031339645386}]\n",
            "fucking service [{'label': '1 star', 'score': 0.5812363624572754}]\n",
            "soso [{'label': '3 stars', 'score': 0.28597524762153625}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # pad_token 지정\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "\n",
        "prompts = [\n",
        "    \"Experience the wonders of interstellar travel with GalactoMail: \",\n",
        "    \"Introducing a revolutionary space communication service: \",\n",
        "    \"Imagine sending emails across galaxies with: \"\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=50,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7,\n",
        "        repetition_penalty=1.3,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # 필요하면 줄바꿈이나 이상한 문자 제거\n",
        "    result = result.strip().split('\\n')[0]\n",
        "    print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQSv53vp2prx",
        "outputId": "d1c1d15f-dd36-4b83-f55e-9a824723b0c5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experience the wonders of interstellar travel with GalactoMail:  a simple, easy to use email client.\n",
            "Introducing a revolutionary space communication service:  Skype.\n",
            "Imagine sending emails across galaxies with:  (a) an atomic cloud, (b)\"\n"
          ]
        }
      ]
    }
  ]
}