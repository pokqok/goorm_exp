{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6Q3ZM_nUzD3",
        "outputId": "f5d3042f-2ff5-4d17-e21b-746171a5a074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4657, 4)\n",
            "   index                      title    genre   \n",
            "0      0          Drowned Wednesday  fantasy  \\\n",
            "1      1              The Lost Hero  fantasy   \n",
            "2      2  The Eyes of the Overworld  fantasy   \n",
            "3      3            Magic's Promise  fantasy   \n",
            "4      4             Taran Wanderer  fantasy   \n",
            "\n",
            "                                             summary  \n",
            "0   Drowned Wednesday is the first Trustee among ...  \n",
            "1   As the book opens, Jason awakens on a school ...  \n",
            "2   Cugel is easily persuaded by the merchant Fia...  \n",
            "3   The book opens with Herald-Mage Vanyel return...  \n",
            "4   Taran and Gurgi have returned to Caer Dallben...  \n"
          ]
        }
      ],
      "source": [
        "# 1. 환경 설정 및 데이터 로드\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 'book_reviews.csv'에는 컬럼 'review', 'genre'가 있습니다.\n",
        "df = pd.read_csv('book_reviews.csv')\n",
        "print(df.shape)      # (샘플 수, 2)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FfsCv4qYUuf",
        "outputId": "b13d19a6-0976-4f6c-8627-83eea0d80ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "토큰화 결과:\n",
            "                                                summary   \n",
            "0      Drowned Wednesday is the first Trustee among ...  \\\n",
            "1      As the book opens, Jason awakens on a school ...   \n",
            "2      Cugel is easily persuaded by the merchant Fia...   \n",
            "3      The book opens with Herald-Mage Vanyel return...   \n",
            "4      Taran and Gurgi have returned to Caer Dallben...   \n",
            "...                                                 ...   \n",
            "4652  Atticus O’Sullivan, last of the Druids, lives ...   \n",
            "4653  Charlie Bucket's wonderful adventure begins wh...   \n",
            "4654  \"I live for the dream that my children will be...   \n",
            "4655  Rose loves Dimitri, Dimitri might love Tasha, ...   \n",
            "4656  The Prince of no value\\nBrishen Khaskem, princ...   \n",
            "\n",
            "                                                 tokens  \n",
            "0     [drowned, wednesday, is, the, first, trustee, ...  \n",
            "1     [as, the, book, opens, jason, awakens, on, a, ...  \n",
            "2     [cugel, is, easily, persuaded, by, the, mercha...  \n",
            "3     [the, book, opens, with, heraldmage, vanyel, r...  \n",
            "4     [taran, and, gurgi, have, returned, to, caer, ...  \n",
            "...                                                 ...  \n",
            "4652  [atticus, osullivan, last, of, the, druids, li...  \n",
            "4653  [charlie, buckets, wonderful, adventure, begin...  \n",
            "4654  [i, live, for, the, dream, that, my, children,...  \n",
            "4655  [rose, loves, dimitri, dimitri, might, love, t...  \n",
            "4656  [the, prince, of, no, value, brishen, khaskem,...  \n",
            "\n",
            "[4657 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "# 2. 텍스트 전처리 및 토큰화\n",
        "def preprocess_text(text):\n",
        "    # 소문자 변환\n",
        "    text = text.lower()\n",
        "\n",
        "    # 한글, 영어, 숫자, 공백만 남기기\n",
        "    text = re.sub(r'[^가-힣a-z0-9\\s]', '', text)\n",
        "\n",
        "    # 토큰화\n",
        "    tokens = text.split()\n",
        "\n",
        "    return tokens\n",
        "\n",
        "df['tokens'] = df['summary'].apply(preprocess_text)\n",
        "print(\"\\n토큰화 결과:\")\n",
        "print(df[['summary', 'tokens']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0QybHqKZFgj",
        "outputId": "7f0a51fd-b60c-4b47-85d9-9e2183cd2f58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.11/dist-packages (1.24.3)\n",
            "Requirement already satisfied: pandas==2.0.1 in /usr/local/lib/python3.11/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.11/dist-packages (2.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.1) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.1) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n",
            "\n",
            "FastText 벡터 차원: 300\n"
          ]
        }
      ],
      "source": [
        "# 3. FastText 임베딩 로드\n",
        "%pip install numpy==1.24.3 pandas==2.0.1 torch==2.0.1 scikit-learn gensim\n",
        "\n",
        "import gensim.downloader as api\n",
        "fasttext = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "print(\"\\nFastText 벡터 차원:\", fasttext.vector_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uPsOY6YjcuPc"
      },
      "outputs": [],
      "source": [
        "# 4. 단어 사전(vocab) 구축 및 시퀀스 변환\n",
        "from collections import Counter\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "all_tokens = [tok for tokens in df['tokens'] for tok in tokens]\n",
        "vocab_count = Counter(all_tokens)\n",
        "\n",
        "# 가장 빈도 높은 MAX_VOCAB_SIZE-1개의 단어를 뽑아 인덱스 매핑 생성\n",
        "word2idx = {'<OOV>': 0}  # idx 0은 OOV용으로 예약\n",
        "for word, count in vocab_count.most_common(MAX_VOCAB_SIZE - 1):\n",
        "    word2idx[word] = len(word2idx)\n",
        "\n",
        "def tokens_to_sequence(tokens, word2idx, maxlen=100):\n",
        "    # 각 토큰을 인덱스로 변환, OOV는 0\n",
        "    seq = [word2idx.get(token, 0) for token in tokens]\n",
        "\n",
        "    # 길이 > maxlen이면 자르고, < maxlen이면 0으로 패딩\n",
        "    if len(seq) > maxlen:\n",
        "        seq = seq[:maxlen]\n",
        "    else:\n",
        "        seq = seq + [0] * (maxlen - len(seq))\n",
        "\n",
        "    return seq\n",
        "\n",
        "df['seq'] = df['tokens'].apply(lambda toks: tokens_to_sequence(toks, word2idx, maxlen=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1LLAy4smc28S"
      },
      "outputs": [],
      "source": [
        "# 5. 학습/검증 데이터 준비\n",
        "import numpy as np\n",
        "X = np.stack(df['seq'].values)              # (N, maxlen)\n",
        "y = df['genre'].factorize()[0]              # [0,1,2,...]\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zgMF7ASIgPHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0dd0b7-8edb-48d2-b3de-737d89cac519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-7ebfa8ae0c0e>:13: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  embedding_matrix[idx] = torch.FloatTensor(fasttext[word])\n"
          ]
        }
      ],
      "source": [
        "# 6. PyTorch 모델 구현\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 6-1) 임베딩 레이어 초기화\n",
        "vocab_size = len(word2idx) + 1\n",
        "embed_dim = fasttext.vector_size\n",
        "\n",
        "embedding_matrix = torch.zeros(vocab_size, embed_dim)\n",
        "# word2idx에 해당하는 fasttext 벡터로 embedding_matrix 채우기\n",
        "for word, idx in word2idx.items():\n",
        "    if word in fasttext.key_to_index and idx != 0:\n",
        "        embedding_matrix[idx] = torch.FloatTensor(fasttext[word])\n",
        "\n",
        "embedding_layer = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "# embedding_layer.weight.data에 embedding_matrix 할당\n",
        "embedding_layer.weight.data.copy_(embedding_matrix)\n",
        "# 임베딩 고정(freeze)\n",
        "embedding_layer.weight.requires_grad = False\n",
        "\n",
        "# 6-2) 분류기 클래스 정의\n",
        "class VanillaFC(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        # 평균 임베딩 벡터 입력 → 예측 로직\n",
        "        x = embedding_layer(x)  # (batch, seq_len, embed_dim)\n",
        "        x = x.mean(dim=1)  # 시퀀스 차원에서 평균 (batch, embed_dim)\n",
        "        return self.fc(x)\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, embedding_layer, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        # embedding_layer 복사\n",
        "        self.embedding = embedding_layer\n",
        "        # nn.RNN 정의\n",
        "        self.rnn = nn.RNN(embedding_layer.embedding_dim, hidden_dim,\n",
        "                          batch_first=True, num_layers=1)\n",
        "        # 마지막 시점 hidden → fc\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 임베딩 적용\n",
        "        x = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
        "        # RNN 처리\n",
        "        _, hidden = self.rnn(x)  # hidden: (1, batch, hidden_dim)\n",
        "        # 마지막 hidden state로 분류\n",
        "        return self.fc(hidden.squeeze(0))\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, embedding_layer, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = embedding_layer\n",
        "        # nn.LSTM 정의\n",
        "        self.lstm = nn.LSTM(embedding_layer.embedding_dim, hidden_dim,\n",
        "                            batch_first=True, num_layers=1)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        # LSTM 처리\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        # 마지막 hidden state로 분류\n",
        "        return self.fc(hidden.squeeze(0))\n",
        "\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, embedding_layer, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = embedding_layer\n",
        "        # nn.GRU 정의\n",
        "        self.gru = nn.GRU(embedding_layer.embedding_dim, hidden_dim,\n",
        "                          batch_first=True, num_layers=1)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        # GRU 처리\n",
        "        _, hidden = self.gru(x)\n",
        "        # 마지막 hidden state로 분류\n",
        "        return self.fc(hidden.squeeze(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aoL0MqdmhVvQ"
      },
      "outputs": [],
      "source": [
        "# 7. 학습 및 평가 함수\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def train(model, optimizer, criterion, loader, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_x, batch_y in loader:\n",
        "        # 배치 데이터를 디바이스로 이동\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "        # 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 순전파\n",
        "        outputs = model(batch_x)\n",
        "\n",
        "        # loss 계산\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "\n",
        "        # 가중치 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 배치 손실 누적\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # 평균 손실 반환\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # 기울기 계산 비활성화\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in loader:\n",
        "            # 배치 데이터를 디바이스로 이동\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "            # 예측\n",
        "            outputs = model(batch_x)\n",
        "\n",
        "            # 최대 확률 클래스 선택\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # 전체 샘플 수 누적\n",
        "            total += batch_y.size(0)\n",
        "\n",
        "            # 정확히 예측한 샘플 수 누적\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "    # 정확도 반환\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvC2rzQEiQRm",
        "outputId": "758b16cc-8451-4bcd-fa25-e049e917cf0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">> Training FC model\n",
            "Epoch 01 — train_loss: 2.0989, val_acc: 0.2200\n",
            "Epoch 02 — train_loss: 2.0078, val_acc: 0.2736\n",
            "Epoch 03 — train_loss: 1.9669, val_acc: 0.2908\n",
            "Epoch 04 — train_loss: 1.9054, val_acc: 0.3187\n",
            "Epoch 05 — train_loss: 1.8416, val_acc: 0.3423\n",
            "Epoch 06 — train_loss: 1.7729, val_acc: 0.4099\n",
            "Epoch 07 — train_loss: 1.7138, val_acc: 0.4088\n",
            "Epoch 08 — train_loss: 1.6556, val_acc: 0.4496\n",
            "Epoch 09 — train_loss: 1.6071, val_acc: 0.4496\n",
            "Epoch 10 — train_loss: 1.5579, val_acc: 0.4678\n",
            "[FC] Final Validation Accuracy: 0.4678\n",
            "\n",
            ">> Training RNN model\n",
            "Epoch 01 — train_loss: 2.0626, val_acc: 0.2200\n",
            "Epoch 02 — train_loss: 2.0220, val_acc: 0.2264\n",
            "Epoch 03 — train_loss: 2.0083, val_acc: 0.2264\n",
            "Epoch 04 — train_loss: 1.9845, val_acc: 0.2414\n",
            "Epoch 05 — train_loss: 1.9563, val_acc: 0.2436\n",
            "Epoch 06 — train_loss: 1.9278, val_acc: 0.2242\n",
            "Epoch 07 — train_loss: 1.8994, val_acc: 0.2586\n",
            "Epoch 08 — train_loss: 1.8909, val_acc: 0.2511\n",
            "Epoch 09 — train_loss: 1.8209, val_acc: 0.2564\n",
            "Epoch 10 — train_loss: 1.7699, val_acc: 0.2135\n",
            "[RNN] Final Validation Accuracy: 0.2135\n",
            "\n",
            ">> Training LSTM model\n",
            "Epoch 01 — train_loss: 2.0713, val_acc: 0.2200\n",
            "Epoch 02 — train_loss: 2.0227, val_acc: 0.2200\n",
            "Epoch 03 — train_loss: 2.0164, val_acc: 0.2318\n",
            "Epoch 04 — train_loss: 1.9944, val_acc: 0.2382\n",
            "Epoch 05 — train_loss: 1.9800, val_acc: 0.2285\n",
            "Epoch 06 — train_loss: 1.9781, val_acc: 0.2876\n",
            "Epoch 07 — train_loss: 1.9617, val_acc: 0.2264\n",
            "Epoch 08 — train_loss: 1.9352, val_acc: 0.2650\n",
            "Epoch 09 — train_loss: 1.9278, val_acc: 0.2393\n",
            "Epoch 10 — train_loss: 1.9007, val_acc: 0.2468\n",
            "[LSTM] Final Validation Accuracy: 0.2468\n",
            "\n",
            ">> Training GRU model\n",
            "Epoch 01 — train_loss: 2.0816, val_acc: 0.2200\n",
            "Epoch 02 — train_loss: 2.0134, val_acc: 0.2296\n",
            "Epoch 03 — train_loss: 1.9850, val_acc: 0.2843\n",
            "Epoch 04 — train_loss: 1.9137, val_acc: 0.2736\n",
            "Epoch 05 — train_loss: 1.7788, val_acc: 0.3090\n",
            "Epoch 06 — train_loss: 1.6230, val_acc: 0.3884\n",
            "Epoch 07 — train_loss: 1.5092, val_acc: 0.4281\n",
            "Epoch 08 — train_loss: 1.4096, val_acc: 0.4217\n",
            "Epoch 09 — train_loss: 1.3641, val_acc: 0.4861\n",
            "Epoch 10 — train_loss: 1.2952, val_acc: 0.4893\n",
            "[GRU] Final Validation Accuracy: 0.4893\n"
          ]
        }
      ],
      "source": [
        "# 8. 모델 학습 및 성능 비교\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# (1) device, num_classes, hidden_dim 재정의\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_classes = len(set(y))    # y는 5번에서 분할된 전체 레이블\n",
        "hidden_dim  = 64\n",
        "\n",
        "# (2) embedding_layer도 device로 이동 (FC 모델에서 사용됨)\n",
        "embedding_layer = embedding_layer.to(device)\n",
        "\n",
        "# DataLoader 준비\n",
        "batch_size = 32\n",
        "train_ds = TensorDataset(\n",
        "    torch.tensor(X_train, dtype=torch.long),\n",
        "    torch.tensor(y_train, dtype=torch.long)\n",
        ")\n",
        "val_ds = TensorDataset(\n",
        "    torch.tensor(X_test,  dtype=torch.long),\n",
        "    torch.tensor(y_test,  dtype=torch.long)\n",
        ")\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 학습 하이퍼파라미터\n",
        "num_epochs    = 10\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# 모델 정의\n",
        "models = {\n",
        "    'FC':   VanillaFC(input_dim=embed_dim, num_classes=num_classes),\n",
        "    'RNN':  RNNClassifier(embedding_layer, hidden_dim, num_classes),\n",
        "    'LSTM': LSTMClassifier(embedding_layer, hidden_dim, num_classes),\n",
        "    'GRU':  GRUClassifier(embedding_layer, hidden_dim, num_classes),\n",
        "}\n",
        "\n",
        "# 학습/검증 루프\n",
        "for name, model in models.items():\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(f\"\\n>> Training {name} model\")\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        train_loss = train(model, optimizer, criterion, train_loader, device)\n",
        "        val_acc    = evaluate(model, val_loader, device)\n",
        "        print(f\"Epoch {epoch:02d} — train_loss: {train_loss:.4f}, val_acc: {val_acc:.4f}\")\n",
        "\n",
        "    final_acc = evaluate(model, val_loader, device)\n",
        "    print(f\"[{name}] Final Validation Accuracy: {final_acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}