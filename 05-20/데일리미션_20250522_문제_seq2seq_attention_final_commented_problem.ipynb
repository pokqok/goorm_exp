{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5680a265",
   "metadata": {},
   "source": [
    "# Seq2Seq + Attention 실습 노트북 (최종 버전)\n",
    "\n",
    "이 노트북은 외부 데이터셋 없이 파이썬 리스트로 정의한 간단한 병렬 코퍼스를 사용해\n",
    "Seq2Seq RNN + Attention 모델을 학습하고 번역 예시를 확인하는 예제입니다.\n",
    "\n",
    "각 셀을 순서대로 실행하여 결과를 확인하세요.\n",
    "\n",
    "모든 주요 단계에 상세한 주석을 추가하여 코드 이해도를 높였습니다.\n",
    "Teacher Forcing 절충(tf_ratio=0.5) 및 hidden/cell 분리 맵핑 적용.\n",
    "\n",
    "문제: 코드 중에서 어텐션 관련된 TODO 부분을 찾아 코드를 완성하고 전체 프로세스를 실행해 보시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730ee170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "### 1) 라이브러리 임포트 및 환경 설정\n",
    "import random  # 시드 고정을 위한 random\n",
    "import numpy as np  # 배열 연산\n",
    "import torch  # PyTorch 기본 모듈\n",
    "import torch.nn as nn  # 신경망 레이어 모듈\n",
    "import torch.optim as optim  # 최적화 도구\n",
    "\n",
    "# 재현성을 위한 시드 고정\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# GPU 사용 시 자동 선택, 없으면 CPU 사용\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0689cb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 문장 쌍 수: 6\n"
     ]
    }
   ],
   "source": [
    "### 2) 병렬 코퍼스 정의\n",
    "# 영어 문장과 대응되는 프랑스어 문장 쌍을 리스트로 정의\n",
    "pairs = [\n",
    "    ('hello',       'bonjour'),    # 인사\n",
    "    ('thank you',   'merci'),      # 감사 표현\n",
    "    ('good night',  'bonne nuit'), # 작별 인사\n",
    "    ('i am hungry', \"j'ai faim\"), # 상태 묘사\n",
    "    ('i love you',  \"je t'aime\"),# 감정 표현\n",
    "    ('how are you', 'comment ça va'), # 질문\n",
    "]\n",
    "print(f\"총 문장 쌍 수: {len(pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cedc2af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 vocab 크기: 14 토큰\n",
      "프랑스어 vocab 크기: 14 토큰\n"
     ]
    }
   ],
   "source": [
    "### 3) 토큰화 및 어휘집 구축\n",
    "def tokenize(sentence):\n",
    "    '''\n",
    "    문자열을 소문자로 변경하고 공백으로 분할하여 토큰 리스트 반환\n",
    "    e.g. 'Hello World' -> ['hello','world']\n",
    "    '''\n",
    "    return sentence.lower().split()\n",
    "\n",
    "# 문장 시작/종료, 패딩 토큰 정의\n",
    "SOS_TOKEN, EOS_TOKEN, PAD_TOKEN = '<sos>', '<eos>', '<pad>'\n",
    "\n",
    "def build_vocab(sent_list):\n",
    "    '''\n",
    "    토큰 리스트에서 어휘집 생성\n",
    "    special token: PAD=0, SOS=1, EOS=2\n",
    "    '''\n",
    "    vocab = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2}\n",
    "    for sent in sent_list:\n",
    "        for tok in tokenize(sent):\n",
    "            if tok not in vocab:\n",
    "                vocab[tok] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# 영어/프랑스어 텍스트 분리\n",
    "src_texts = [src for src, _ in pairs]\n",
    "trg_texts = [trg for _, trg in pairs]\n",
    "# 어휘집 생성\n",
    "SRC_VOCAB = build_vocab(src_texts)\n",
    "TRG_VOCAB = build_vocab(trg_texts)\n",
    "# 인덱스->토큰 매핑 생성\n",
    "SRC_IVAL = {i:tok for tok,i in SRC_VOCAB.items()}\n",
    "TRG_IVAL = {i:tok for tok,i in TRG_VOCAB.items()}\n",
    "\n",
    "print(f\"영어 vocab 크기: {len(SRC_VOCAB)} 토큰\")\n",
    "print(f\"프랑스어 vocab 크기: {len(TRG_VOCAB)} 토큰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b48e1cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_batch shape: torch.Size([5, 6])\n",
      "trg_batch shape: torch.Size([5, 6])\n"
     ]
    }
   ],
   "source": [
    "### 4) 인코딩 및 패딩\n",
    "# 최대 시퀀스 길이 (SOS, EOS 포함)\n",
    "MAX_SRC_LEN = max(len(tokenize(s)) + 2 for s in src_texts)\n",
    "MAX_TRG_LEN = max(len(tokenize(s)) + 2 for s in trg_texts)\n",
    "\n",
    "def encode_and_pad(sentence, vocab, max_len):\n",
    "    '''\n",
    "    문장 -> 인덱스 리스트 변환\n",
    "    [SOS] + tokens + [EOS] + [PAD...]\n",
    "    '''\n",
    "    tokens = tokenize(sentence)\n",
    "    idxs = [vocab[SOS_TOKEN]] + [vocab[t] for t in tokens] + [vocab[EOS_TOKEN]]\n",
    "    # 부족한 길이만큼 PAD 추가\n",
    "    idxs += [vocab[PAD_TOKEN]] * (max_len - len(idxs))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "import torch\n",
    "# 배치 데이터 생성 (모든 예시 사용)\n",
    "dataset = [\n",
    "    (encode_and_pad(s, SRC_VOCAB, MAX_SRC_LEN),\n",
    "     encode_and_pad(t, TRG_VOCAB, MAX_TRG_LEN))\n",
    "    for s, t in pairs\n",
    "]\n",
    "# 텐서로 변환 후 차원 Transpose: [batch, seq] -> [seq, batch]\n",
    "src_batch = torch.stack([d[0] for d in dataset]).t().to(DEVICE)  # [src_len, batch]\n",
    "trg_batch = torch.stack([d[1] for d in dataset]).t().to(DEVICE)  # [trg_len, batch]\n",
    "print('src_batch shape:', src_batch.shape)\n",
    "print('trg_batch shape:', trg_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5) 모델 정의\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    양방향 LSTM 인코더\n",
    "    '''\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        # 임베딩 레이어: 인덱스 -> 벡터\n",
    "        self.embed = nn.Embedding(input_dim, emb_dim)\n",
    "        # 2-way LSTM\n",
    "        self.rnn   = nn.LSTM(emb_dim, hid_dim, bidirectional=True)\n",
    "        # hidden, cell 분리 맵핑\n",
    "        self.fc_h  = nn.Linear(hid_dim*2, hid_dim)\n",
    "        self.fc_c  = nn.Linear(hid_dim*2, hid_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: [seq_len, batch]\n",
    "        embedded = self.embed(src)  # [seq_len, batch, emb_dim]\n",
    "        outputs, (h, c) = self.rnn(embedded)\n",
    "        # 양방향 마지막 레이어 concat\n",
    "        h_cat = torch.cat((h[-2], h[-1]), dim=1)  # [batch, hid_dim*2]\n",
    "        c_cat = torch.cat((c[-2], c[-1]), dim=1)\n",
    "        # 맵핑 후 unsqueeze\n",
    "        h0 = torch.tanh(self.fc_h(h_cat)).unsqueeze(0)  # [1, batch, hid_dim]\n",
    "        c0 = torch.tanh(self.fc_c(c_cat)).unsqueeze(0)\n",
    "        return outputs, h0, c0\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Bahdanau Attention\n",
    "    '''\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hid_dim*3, hid_dim)\n",
    "        self.v    = nn.Linear(hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, enc_outs):\n",
    "        # hidden: [1, batch, hid_dim]\n",
    "        # enc_outs: [src_len, batch, hid_dim*2]\n",
    "        src_len, batch, _ = enc_outs.shape\n",
    "        # repeat hidden across src_len\n",
    "        hidden_rep = hidden.repeat(src_len, 1, 1)  # [src_len, batch, hid_dim]\n",
    "        # energy: alignment scores\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden_rep, enc_outs), dim=2)))\n",
    "        # compute attention weights\n",
    "        return torch.softmax(self.v(energy).squeeze(2), dim=0)  # [src_len, batch]\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    어텐션 기반 디코더\n",
    "    '''\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, attention):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn   = nn.LSTM(hid_dim*2 + emb_dim, hid_dim)\n",
    "        self.attn  = attention\n",
    "        self.out   = nn.Linear(hid_dim*3 + emb_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell, enc_outs):\n",
    "        # input: [batch] 단일 timestep\n",
    "        input = input.unsqueeze(0)  # [1, batch]\n",
    "        emb   = self.embed(input)  # [1, batch, emb_dim]\n",
    "        # 어텐션 가중치\n",
    "        # TODO\n",
    "        # reshape for bmm\n",
    "        a     = a.unsqueeze(1).permute(2, 1, 0)  # [batch,1,src_len]\n",
    "        enc   = enc_outs.permute(1, 0, 2)       # [batch,src_len,hid_dim*2]\n",
    "        # context vector\n",
    "        ctx   = torch.bmm(a, enc).permute(1, 0, 2)  # [1,batch,hid_dim*2]\n",
    "        # concatenate emb and context\n",
    "        # TODO\n",
    "        # TODO\n",
    "        output = output.squeeze(0)  # [batch, hid_dim]\n",
    "        emb    = emb.squeeze(0)     # [batch, emb_dim]\n",
    "        ctx    = ctx.squeeze(0)     # [batch, hid_dim*2]\n",
    "        # 최종 예측\n",
    "        pred   = self.out(torch.cat((output, ctx, emb), dim=1))  # [batch, output_dim]\n",
    "        return pred, h, c\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    '''\n",
    "    Encoder-Decoder 통합\n",
    "    '''\n",
    "    def __init__(self, encoder, decoder, device, tf_ratio=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder  = encoder\n",
    "        self.decoder  = decoder\n",
    "        self.device   = device\n",
    "        self.tf_ratio = tf_ratio  # teacher forcing 비율\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # src: [src_len, batch], trg: [trg_len, batch]\n",
    "        trg_len, batch = trg.shape\n",
    "        vocab_size    = self.decoder.out.out_features\n",
    "        # 저장용 텐서 초기화\n",
    "        outputs       = torch.zeros(trg_len, batch, vocab_size).to(self.device)\n",
    "        # 인코더 실행\n",
    "        enc_outs, h, c = self.encoder(src)\n",
    "        # 첫 입력 토큰: <sos>\n",
    "        input_tok      = trg[0]\n",
    "        for t in range(1, trg_len):\n",
    "            # 디코더 한 스텝 실행\n",
    "            pred, h, c = self.decoder(input_tok, h, c, enc_outs)\n",
    "            outputs[t] = pred\n",
    "            # teacher forcing: 절반 확률로 정답 사용\n",
    "            teacher = random.random() < self.tf_ratio\n",
    "            input_tok = trg[t] if teacher else pred.argmax(1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f740bc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embed): Embedding(14, 32)\n",
      "    (rnn): LSTM(32, 64, bidirectional=True)\n",
      "    (fc_h): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (fc_c): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embed): Embedding(14, 32)\n",
      "    (rnn): LSTM(160, 64)\n",
      "    (attn): Attention(\n",
      "      (attn): Linear(in_features=192, out_features=64, bias=True)\n",
      "      (v): Linear(in_features=64, out_features=1, bias=False)\n",
      "    )\n",
      "    (out): Linear(in_features=224, out_features=14, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### 6) 모델 초기화 및 학습 설정\n",
    "INPUT_DIM = len(SRC_VOCAB)\n",
    "OUTPUT_DIM= len(TRG_VOCAB)\n",
    "ENC_EMB_DIM, DEC_EMB_DIM, HID_DIM = 32, 32, 64\n",
    "\n",
    "encoder  = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM)\n",
    "attention= Attention(HID_DIM)\n",
    "decoder  = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, attention)\n",
    "model    = Seq2Seq(encoder, decoder, DEVICE, tf_ratio=0.5).to(DEVICE)\n",
    "\n",
    "optimizer= optim.Adam(model.parameters(), lr=1e-3)\n",
    "PAD_IDX  = TRG_VOCAB[PAD_TOKEN]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb06a013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 | Loss: 0.0206\n",
      "Epoch 200 | Loss: 0.0050\n",
      "Epoch 300 | Loss: 0.0024\n",
      "Epoch 400 | Loss: 0.0015\n",
      "Epoch 500 | Loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "### 7) 학습 루프 (500 에폭)\n",
    "model.train()\n",
    "for epoch in range(1, 501):\n",
    "    optimizer.zero_grad()\n",
    "    outputs  = model(src_batch, trg_batch)\n",
    "    # Loss 계산: 예측과 정답 reshape\n",
    "    preds    = outputs[1:].reshape(-1, outputs.shape[-1])\n",
    "    targets  = trg_batch[1:].reshape(-1)\n",
    "    loss     = criterion(preds, targets)\n",
    "    loss.backward()  # 역전파\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)  # gradient clip\n",
    "    optimizer.step()  # 파라미터 업데이트\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab43f4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello        → bonjour\n",
      "thank you    → merci\n",
      "good night   → bonne nuit\n",
      "i am hungry  → j'ai faim\n",
      "i love you   → je t'aime\n",
      "how are you  → comment ça va\n"
     ]
    }
   ],
   "source": [
    "### 8) 번역 예시 및 결과 확인\n",
    "def translate(sentence):\n",
    "    '''\n",
    "    한 문장을 입력받아 번역 결과 반환\n",
    "    '''\n",
    "    model.eval()\n",
    "    tokens = tokenize(sentence)\n",
    "    # input tensor 생성\n",
    "    idxs   = [SRC_VOCAB[SOS_TOKEN]] + [SRC_VOCAB.get(t, 0) for t in tokens] + [SRC_VOCAB[EOS_TOKEN]]\n",
    "    tensor = torch.LongTensor(idxs).unsqueeze(1).to(DEVICE)\n",
    "    # 인코더 실행\n",
    "    enc_outs, h, c = encoder(tensor)\n",
    "    trg_idxs = [TRG_VOCAB[SOS_TOKEN]]\n",
    "    for _ in range(MAX_TRG_LEN):\n",
    "        prev = torch.LongTensor([trg_idxs[-1]]).to(DEVICE)\n",
    "        pred, h, c = decoder(prev, h, c, enc_outs)\n",
    "        nxt = pred.argmax(1).item()\n",
    "        trg_idxs.append(nxt)\n",
    "        if nxt == TRG_VOCAB[EOS_TOKEN]:\n",
    "            break\n",
    "    # index -> token 변환 및 문자열 반환\n",
    "    return ' '.join([TRG_IVAL[i] for i in trg_idxs[1:-1]])\n",
    "\n",
    "for src, _ in pairs:\n",
    "    print(f\"{src:12s} → {translate(src)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
